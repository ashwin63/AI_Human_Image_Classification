# -*- coding: utf-8 -*-
"""CV_Project_V2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CDdvCbxw1DfGLA-cW7TrG9ukQa18xQhg
"""

!pip --version

#prerequisites
!pip install datasets transformers torch
!pip install accelerate -U

!git clone https://github.com/ashwin63/AI_Human_Image_Classification.git

import numpy as np
from datasets import load_dataset
from transformers import ViTFeatureExtractor
import torch
from transformers import TrainingArguments
from transformers import Trainer
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator

from transformers import ViTForImageClassification, default_data_collator
from torch.utils.data import DataLoader, Dataset
import os
from PIL import Image

print(torch.__version__)
import transformers, tensorflow, numpy
print(transformers.__version__)
print(tensorflow.__version__)
print(numpy.__version__)

class CustomImageDataset(Dataset):
    def __init__(self, img_dir, feature_extractor):
        self.img_dir = img_dir
        self.img_labels = []
        self.img_files = []
        self.feature_extractor = feature_extractor
        self.label_mapping = {'REAL': 1, 'FAKE': 0}

        for label_dir in ['REAL', 'FAKE']:
            dir_path = os.path.join(img_dir, label_dir)
            files = os.listdir(dir_path)
            for file in files:
                self.img_files.append(os.path.join(dir_path, file))
                self.img_labels.append(self.label_mapping[label_dir])

    def __len__(self):
        return len(self.img_files)

    def __getitem__(self, idx):
        img_path = self.img_files[idx]
        image = Image.open(img_path).convert("RGB")
        label = self.img_labels[idx]
        features = self.feature_extractor(images=image, return_tensors="pt")
        # Ensure the output is in a dictionary format
        return {"pixel_values": features['pixel_values'].squeeze(), "labels": torch.tensor(label)}

model_id = 'google/vit-base-patch16-224-in21k'
feature_extractor = ViTFeatureExtractor.from_pretrained(
    model_id
)
# device will determine whether to run the training on GPU or CPU.
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device

model = ViTForImageClassification.from_pretrained(
    model_id,  # classification head
    num_labels=2
)
model.to(device)

# Initialize the feature extractor
feature_extractor = ViTFeatureExtractor.from_pretrained(model_id)

# Create the dataset
train_dataset = CustomImageDataset(img_dir='/content/AI_Human_Image_Classification/dataset/train', feature_extractor=feature_extractor)
test_dataset = CustomImageDataset(img_dir='/content/AI_Human_Image_Classification/dataset/test', feature_extractor=feature_extractor)

# Create the DataLoader
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)

training_args = TrainingArguments(
    max_steps = 100,
    output_dir="./cifar",
    per_device_train_batch_size=4,
    evaluation_strategy="steps",
    num_train_epochs=4,
    save_steps=100,
    eval_steps=100,
    logging_steps=10,
    learning_rate=2e-4,
    save_total_limit=2,
    remove_unused_columns=False,
    push_to_hub=False,
    load_best_model_at_end=True,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    data_collator=default_data_collator,
    compute_metrics=None,  # Define your metrics function if needed
)

trainer.train()

model_path = "path/to/save/model"
model.save_pretrained(model_path)
feature_extractor.save_pretrained(model_path)